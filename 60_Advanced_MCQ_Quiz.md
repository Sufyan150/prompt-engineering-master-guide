# ğŸ§  60 Advanced Conceptual MCQs on Prompt Engineering
_By [Sufyan Abbasi](https://github.com/Sufyan150)_

---

## ğŸ“˜ Overview
This document contains **60 expert-level multiple-choice questions** designed to test advanced conceptual understanding of **Prompt Engineering**, **LLM reasoning**, **MoE architectures**, and **structured prompting techniques**.  

Difficulty Level: ğŸ”¥ Very High  
Focus: Conceptual mastery, not memorization.

Each question has **four options (Aâ€“D)** and a detailed **answer key** at the end.

---

## ğŸ§© Multiple-Choice Questions

1ï¸âƒ£ **Instructional Locality** refers to:  
A. Random distribution of prompt parts across contexts.  
B. Organizing sub-instructions close to main directives to avoid contextual drift.  
C. Placing commands at the end of the prompt.  
D. Removing local variables to reduce bias.  

---

2ï¸âƒ£ **Intent Collapse** occurs when:  
A. A model merges multiple unrelated goals into a single vague task.  
B. A prompt has redundant context.  
C. The model refuses an instruction.  
D. The system role overrides user messages.

---

3ï¸âƒ£ To enforce *stepwise reasoning*, the best structure is:  
A. Ask for â€œchain-of-thought reasoning.â€  
B. Explicitly number the reasoning steps and request outputs for each stage.  
C. Lower the temperature to zero.  
D. Add examples only.

---

4ï¸âƒ£ **Hallucination** in language models describes:  
A. Intentional creativity in storytelling.  
B. Generation of plausible but incorrect or unverifiable facts.  
C. Token truncation errors.  
D. Missing context window.

---

5ï¸âƒ£ Failure to define a JSON schema often results in:  
A. Modelâ€™s context reset.  
B. Ambiguous field names causing inconsistent structures.  
C. Lower inference speed.  
D. Semantic overfitting.

---

6ï¸âƒ£ Few-shot examples must:  
A. Be lexically complex.  
B. Represent the desired input-output distribution.  
C. Be identical across tasks.  
D. Be semantically ambiguous.

---

7ï¸âƒ£ *Self-consistency prompting* improves reasoning by:  
A. Running multiple reasoning chains and selecting the majority outcome.  
B. Lowering temperature to stabilize output.  
C. Reducing prompt length.  
D. Ignoring few-shot examples.

---

8ï¸âƒ£ Over-specifying constraints in prompts:  
A. Increases creativity.  
B. Reduces diversity and hides novel solutions.  
C. Improves factuality automatically.  
D. Removes hallucination entirely.

---

9ï¸âƒ£ â€œYou are an expertâ€¦â€ roleplay primarily affects:  
A. Tokenization.  
B. Tone, expertise level, and assumptions.  
C. Inference time.  
D. Grammar accuracy.

---

10ï¸âƒ£ When debugging missing JSON fields, you should:  
A. Provide explicit schema and sample output.  
B. Lower temperature.  
C. Add irrelevant context.  
D. Switch to system prompts only.

---

11ï¸âƒ£ *Instructional Primacy* means:  
A. System prompts are ignored.  
B. Early instructions carry higher influence.  
C. The last message always dominates.  
D. Random token prioritization occurs.

---

12ï¸âƒ£ To mitigate bias in LLMs:  
A. Inject neutrality constraints or adversarial checks in prompts.  
B. Add unrelated data.  
C. Increase model size.  
D. Lower temperature below zero.

---

13ï¸âƒ£ Chain-of-thought reasoning enhances:  
A. Simple summarization.  
B. Multi-step logical tasks requiring decomposition.  
C. Grammar correction only.  
D. Random creativity.

---

14ï¸âƒ£ To ground a modelâ€™s answers in truth:  
A. Provide factual data in context and instruct the model to cite it.  
B. Use higher temperature.  
C. Request creativity mode.  
D. Disable reasoning.

---

15ï¸âƒ£ Measuring **consistency** involves:  
A. BLEU score only.  
B. Inter-output agreement across samples.  
C. Counting tokens.  
D. Measuring prompt length.

---

16ï¸âƒ£ Explicit stepwise prompting can reduce performance when:  
A. The task is intuitive or associative, not logical.  
B. Temperature is too low.  
C. Context is long.  
D. Few-shot examples are missing.

---

17ï¸âƒ£ **Prompt Chaining** refers to:  
A. Sequencing multiple prompts where one output feeds the next.  
B. Combining prompts into one message.  
C. Randomly repeating prompts.  
D. Encoding tokens.

---

18ï¸âƒ£ Extraction prompts need:  
A. Ambiguous instructions.  
B. Clear structure, examples, and strict output rules.  
C. High creativity.  
D. Minimal logic.

---

19ï¸âƒ£ *Prompt Injection* is best mitigated by:  
A. Sanitizing user inputs and isolating system messages.  
B. Removing context.  
C. Adding more examples.  
D. Using higher temperature.

---

20ï¸âƒ£ Diverse few-shot examples primarily improve:  
A. Randomness.  
B. Generalization to new inputs.  
C. Latency.  
D. Context length.

---

21ï¸âƒ£ **Red-team prompting** is used to:  
A. Test for harmful outputs and robustness.  
B. Improve summarization.  
C. Enhance creativity.  
D. Debug formatting.

---

22ï¸âƒ£ *Output Anchoring* means:  
A. Fixing token count.  
B. Giving an example output as a structural reference.  
C. Using long prompts.  
D. Anchoring embeddings only.

---

23ï¸âƒ£ To improve numeric accuracy:  
A. Provide formulas and ask the model to show work.  
B. Raise temperature.  
C. Use creativity mode.  
D. Add roleplay.

---

24ï¸âƒ£ Adding citations helps reduce hallucination because:  
A. It narrows the generative space to factual sources.  
B. It extends token length.  
C. It disables reasoning.  
D. It lowers cost.

---

25ï¸âƒ£ Very low temperature may:  
A. Reduce creativity and trap model in high-probability phrasing.  
B. Increase diversity.  
C. Produce long answers.  
D. Decrease coherence.

---

26ï¸âƒ£ Reliable code generation requires:  
A. Constraints, input/output examples, and test cases.  
B. Roleplay only.  
C. Zero-shot approach.  
D. High temperature.

---

27ï¸âƒ£ *Speculative extrapolation* is dangerous when:  
A. Used in safety-critical or factual domains.  
B. Used for fiction.  
C. Context is short.  
D. Chain-of-thought is active.

---

28ï¸âƒ£ To get structured plans:  
A. Ask for enumerated lists with defined steps.  
B. Use random storytelling.  
C. Request poems.  
D. Omit logic.

---

29ï¸âƒ£ Ambiguous pronouns cause:  
A. Context truncation.  
B. Misreferenced subjects and wrong assumptions.  
C. Shorter outputs.  
D. Random tone shifts.

---

30ï¸âƒ£ Robust evaluation includes:  
A. Sampling multiple runs and analyzing metrics.  
B. Visual inspection only.  
C. Longer prompts only.  
D. Ignoring failed outputs.

---

31ï¸âƒ£ *Counterfactual examples* in few-shot prompting show:  
A. Negative or edge cases the model must avoid.  
B. Redundant content.  
C. Noise.  
D. Random examples.

---

32ï¸âƒ£ For multilingual outputs:  
A. Provide examples and specify output language explicitly.  
B. Let model auto-detect.  
C. Use English only.  
D. Add random text.

---

33ï¸âƒ£ System messages serve as:  
A. High-level guardrails with higher priority.  
B. Optional decorative text.  
C. Tokens for randomness.  
D. Bias amplifiers.

---

34ï¸âƒ£ *Reflection prompting* improves outputs by:  
A. Asking model to critique and refine its own answers.  
B. Reducing context.  
C. Adding examples.  
D. Decreasing temperature.

---

35ï¸âƒ£ Retrieval-Augmented Generation fails when:  
A. Retrieved sources are irrelevant or outdated.  
B. Model asks questions.  
C. Prompts are long.  
D. Temperature is high.

---

36ï¸âƒ£ Legal topic prompts should:  
A. Include disclaimers and request informational summaries only.  
B. Request legal decisions.  
C. Omit citations.  
D. Guarantee correctness.

---

37ï¸âƒ£ To reduce verbosity:  
A. Instruct â€œBe concise. Use bullet points and essential facts only.â€  
B. Raise temperature.  
C. Add examples.  
D. Use longer prompts.

---

38ï¸âƒ£ Reproducibility improves with:  
A. Low temperature and fixed random seed.  
B. Changing parameters.  
C. Adding creativity.  
D. Increasing diversity.

---

39ï¸âƒ£ **Memorization** happens when:  
A. Model repeats training data verbatim.  
B. It generates new content.  
C. It reasons logically.  
D. It simplifies answers.

---

40ï¸âƒ£ Prevent truncation issues by:  
A. Placing core instructions at the beginning of context.  
B. Adding them at the end.  
C. Using random formatting.  
D. Adding images.

---

41ï¸âƒ£ High cognitive load occurs when:  
A. Prompt includes multiple unrelated tasks.  
B. Task is simple.  
C. Output is short.  
D. Context is minimal.

---

42ï¸âƒ£ Summarization quality is best measured via:  
A. ROUGE or semantic similarity plus human evaluation.  
B. Word count.  
C. Latency.  
D. Token usage.

---

43ï¸âƒ£ Use â€œAssume X is trueâ€ when:  
A. Conducting hypothetical reasoning.  
B. Seeking factual reports.  
C. Removing logic.  
D. Avoiding context.

---

44ï¸âƒ£ To prevent refusals on valid prompts:  
A. Provide benign intent and contextual justification.  
B. Trick model with adversarial input.  
C. Increase randomness.  
D. Remove system prompts.

---

45ï¸âƒ£ To create tabular outputs:  
A. Request CSV/JSON with example rows.  
B. Use prose.  
C. Avoid format instructions.  
D. Use creative tone.

---

46ï¸âƒ£ Chain-of-thought increases interpretability by:  
A. Showing intermediate reasoning steps for audit.  
B. Compressing tokens.  
C. Randomizing steps.  
D. Removing logic.

---

47ï¸âƒ£ To boost factuality:  
A. Retrieve trusted documents and ask model to cite them.  
B. Increase creativity.  
C. Skip context.  
D. Add noise.

---

48ï¸âƒ£ â€œGive 3 alternative perspectivesâ€ improves:  
A. Reasoning breadth and solution diversity.  
B. Speed.  
C. Token compression.  
D. Determinism.

---

49ï¸âƒ£ Hallucination detection cue:  
A. Confident tone with unverifiable details.  
B. Repetition.  
C. Slow speed.  
D. Grammar error.

---

50ï¸âƒ£ Calibration means:  
A. Modelâ€™s confidence matches empirical accuracy (70% = ~70% correct).  
B. Always 100% certainty.  
C. Random token control.  
D. Reduced creativity.

---

51ï¸âƒ£ To reduce formatting errors:  
A. Provide explicit examples or templates.  
B. Keep vague instructions.  
C. Add random context.  
D. Raise temperature.

---

52ï¸âƒ£ Diagnostic prompts help to:  
A. Identify model weaknesses and sensitivity to phrasing.  
B. Generate fiction.  
C. Increase token usage.  
D. Lower inference time.

---

53ï¸âƒ£ Iterative prompting is superior because:  
A. It enables incremental refinement and alignment.  
B. Itâ€™s faster.  
C. It lowers cost only.  
D. It eliminates bias.

---

54ï¸âƒ£ **Emergent capability** means:  
A. Abrupt performance increase at scale not explained by smaller models.  
B. Linear growth.  
C. Reduced latency.  
D. Lower entropy.

---

55ï¸âƒ£ Too many few-shot examples may:  
A. Consume context space and confuse consistency.  
B. Always help.  
C. Reduce variety.  
D. Increase reasoning.

---

56ï¸âƒ£ *Socratic prompting* technique:  
A. Uses guided questions to elicit reasoning.  
B. Randomly outputs answers.  
C. Increases temperature.  
D. Simplifies task.

---

57ï¸âƒ£ To reduce repetition:  
A. Encourage alternate phrasing, adjust sampling.  
B. Fix temperature at 0.  
C. Use same prompt again.  
D. Add roleplay.

---

58ï¸âƒ£ **Ablation** means:  
A. Systematically removing prompt parts to diagnose effect.  
B. Adding more examples.  
C. Using longer prompts.  
D. Random restarts.

---

59ï¸âƒ£ To follow multi-step procedures effectively:  
A. Decompose into ordered substeps and validate each step.  
B. Use creative instructions.  
C. Provide minimal details.  
D. Remove order.

---

60ï¸âƒ£ To avoid prompt leakage with user input:  
A. Sanitize inputs before adding to system prompts.  
B. Append raw data directly.  
C. Skip validation.  
D. Allow unescaped input.

---

## ğŸ§¾ Answer Key

| Q | Ans | Q | Ans | Q | Ans | Q | Ans |
|--:|:--:|--:|:--:|--:|:--:|--:|:--:|
|1|B|2|A|3|B|4|B|
|5|B|6|B|7|A|8|B|
|9|B|10|A|11|B|12|A|
|13|B|14|A|15|B|16|A|
|17|A|18|B|19|A|20|B|
|21|A|22|B|23|A|24|A|
|25|A|26|A|27|A|28|A|
|29|B|30|A|31|A|32|A|
|33|A|34|A|35|A|36|A|
|37|A|38|A|39|A|40|A|
|41|A|42|A|43|A|44|A|
|45|A|46|A|47|A|48|A|
|49|A|50|A|51|A|52|A|
|53|A|54|A|55|A|56|A|
|57|A|58|A|59|A|60|A|

---

## ğŸ’¡ Final Advice
> â€œDonâ€™t memorize â€” *internalize*.  
Prompt engineering mastery lies in understanding how models reason, not just how they respond.â€

---

**Created by [Sufyan Abbasi](https://github.com/Sufyan150)**  
_Open Source for AI Learners Worldwide ğŸŒ_
